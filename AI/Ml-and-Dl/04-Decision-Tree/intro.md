# 4.1 决策树/判定树

1. 决策树（Decision Tree）是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。
2. 决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：
   1. 每个非叶节点表示一个特征属性测试。
   2. 每个分支代表这个特征属性在某个值域上的输出。
   3. 每个叶子节点存放一个类别。
   4. 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。
3. 决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的分治策略。
   ![2.png](https://i.loli.net/2018/10/17/5bc728ecc27fe.png)
4. 显然，决策树的生成是一个递归过程.在决策树基本算法中，有三种情形会导致递归返回
   1. 当前结点包含的样本全属于同一类别，无需划分；
   2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分;
   3. 当前结点包含的样本集合为空，不能划分。
   4. 在第2种情形下，我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；在第3种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。注意这两种情形的处理实质不同：情形2是在利用当前结点的后验分布，而情形3则是把父结点的样本分布作为当前结点的先验分布。
5. 决策树学习的关键是第8行，即如何选择最优划分属性。一般而言，我们希望决策数的分支结点的“纯度”越来越高。

# 4.2 决策树的构造

## 前置知识

- **信息熵**：信息熵(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：

$$
   \text{Ent}(D)=-\sum_{k=1}^{|y|}p_k\log_2p_k
$$

   这里，值越大表示越混乱，而易知只有一个类别时，信息熵为0。（约定：当$p=0$时，$\log_2p=0$）
- **条件信息熵**：条件信息熵$H(X|Y)$是在已知随机变量$Y$的条件下，随机变量$X$的不确定性：

$$
   H(X|Y)=-\sum_{y\in Y} P(y)\sum_{x\in X} P(X|Y)\log P(X|Y)
$$

   这里，p(x|y) 是在已知 Y 的值的情况下 X 取特定值的条件概率，而 p(y) 是 Y 取值的概率。这个公式考虑了所有 Y 的可能值，以及在每个 Y 的值下 X 的所有可能值。
- **信息增益**：假定离散属性$a$有$V$个可能的取值，若使用$a$来对样本集$D$进行划分，则会产生$V$个分支节点，其中第v个分支节点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。信息增益是知道了某个条件后，事件的不确定性下降的程度。

$$
   \text{Gain}(D,a)=\text{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\text{Ent}(D^v)
$$

   信息增益越大，说明使用属性$a$划分样本集$D$所获得的纯度提升越大。

- **信息增益比**：信息增益比（Gain Ratio）是一种用于选择最优划分属性的准则。它是对信息增益（Information Gain）的一种改进，解决了信息增益倾向于选择取值多的属性的问题。其计算公式如下：

$$
   \text{GainRatio}(D,a)=\frac{\text{Gain}(D,a)}{\text{IV}(a)}
$$

   其中，IV(a) 是属性 a 的固有值（Intrinsic Value），定义为：

$$
   \text{IV}(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}
$$

   信息增益比的优点是对可取值数目较少的属性有所偏好，因此可以用来减小信息增益的倾向。

- **基尼指数**：基尼指数（Gini index）是另一种常用的选择最优划分属性的准则。基尼指数是指在样本集合D中，随机抽取两个样本，其类别标记不一致的概率。基尼指数越小，样本集合纯度越高。

$$
   \text{Gini}(D)=\sum_{k=1}^{|y|}\sum_{k'\neq k}p_kp_{k'}=1-\sum_{k=1}^{|y|}p_k^2
$$

   属性a的基尼指数定义为：

$$
   \text{GiniIndex}(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}\text{Gini}(D^v)
$$

   基尼指数越小，说明使用属性$a$划分样本集$D$所获得的纯度提升越大。于是，我们可以通过最小化基尼指数来选择最优划分属性。

## 相关算法

主要有三种算法构造决策树：ID3、C4.5和CART。

1. **ID3算法**：ID3算法是最早提出的决策树学习算法，其核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。ID3算法通过计算每个属性的信息增益，认为信息增益高的是好属性，每次划分选取信息增益最高的属性为划分标准，重复这个过程，直至生成一个能完美分类训练样例的决策树。

   ID3算法的不足之处在于：

   1. ID3算法使用信息增益来选择特征，存在偏向于选择取值较多的特征的问题。
   2. ID3算法无法处理连续特征。
   3. ID3算法无法处理缺失值。
   4. ID3算法生成的树可能过于复杂，容易过拟合。

2. **C4.5算法**：C4.5算法是ID3算法的改进版本，其核心是在决策树各个结点上应用信息增益比准则选择特征，递归地构建决策树。

   C4.5算法的不足之处在于：

   1. C4.5算法在处理缺失值时，使用了启发式方法。
   2. C4.5算法生成的树可能过于复杂，容易过拟合。

   > **启发式算法**是一种在解决复杂问题时用于寻找可行解的方法，尤其是在完全搜索不可行或太耗时的情况下。这些算法通常基于经验或直觉来做出决策，而不是进行穷举搜索。
   >
   > 例如，**贪心算法**是一种常见的启发式算法，它在每一步选择当前看起来最优的选项，而不考虑更大范围的可能性。其他著名的启发式算法包括**模拟退火算法**和**遗传算法**。

3. **CART算法**：CART（Classification And Regression Tree）算法既可以用于分类问题，也可以用于回归问题。CART算法的核心是在决策树各个结点上应用基尼指数选择特征，递归地构建决策树。

   CART算法的不足之处在于：

   1. CART算法生成的是二叉树。
   2. CART算法生成的树可能过于复杂，容易过拟合。