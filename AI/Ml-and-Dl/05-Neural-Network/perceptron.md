# 5.2 感知机与多层网络

## 感知机

感知机是由美国学者 Frank Rosenblatt 在1957年提出来的。感知机是作为神经网络（深度学习）的起源的算法。因此，学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。
感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电流不同的是，感知机的信号只有“流/不流”（1/0）两种取值。这里我们认为0对应“不传递信号”， 1对应“传递信号”。
下图1就是一个接收三个输入信号的感知机的例子。

![img](http://neuralnetworksanddeeplearning.com/images/tikz0.png)

$x_1,x_2,x_3$是输入信号，$y$是输出信号，$w_1,w_2,w_3$是权重（w是weight的首字母）。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重 $w_ix_i$ 。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号 $\theta$ 表示。
把上述内容用数学式来表示，就是

$$
	y=\left\{\begin{matrix} 
	0,\text{if }\sum_jw_jx_j\le\theta\\ 1,\text{if }\sum_j w_jx_j>\theta
	\end{matrix}\right.
$$

感知机可以轻松地实现逻辑与、或、非运算，只需要满足信号总和和阈值的多种关系，即可实现不同的逻辑门。

感知机权重的学习规则如下：对于训练样本（x，y），当该样本进入感知机学习后，会产生一个输出值，若该输出值与样本的真实标记不一致，则感知机会对权重进行调整，若激活函数为阶跃函数，则调整的方法为（基于梯度下降法）：

对于样本  (x, y) , 其预测值为: 

$$
	\hat{\mathrm{y}}=\mathrm{f}\left(\sum_{i=1}^{n} \omega_{i} x_{i}-\theta\right)=\mathrm{f}\left(\sum_{i=1}^{n+1} \omega_{i} x_{i}\right)
$$

其中  $\mathrm{x}_{i+1}=-1$为固定值，均方误差为: $E=\frac{1}{2}(\mathrm{y}-\hat{\mathrm{y}})^{2}$ 。使用梯度下降法寻找最小的均方误差$ \min E $，负的梯度方向为最速下降方向

$$
	\frac{\partial E}{\partial \omega_{1}}=-(\mathrm{y}-\hat{\mathrm{y}}) \frac{\partial \hat{\mathrm{y}}}{\partial \omega_{i}}=-(\mathrm{y}-\hat{\mathrm{y}}) \frac{\partial f\left(\sum_{i=1}^{n+1} \omega_{i} x_{i}\right)}{\partial \omega_{i}}
$$

因为函数$f$为阶跃函数, 故有: 

$$
	\frac{\partial f\left(\sum_{i=1}^{n+1} \omega_{i} x_{i}\right)}{\partial \omega_{i}}=x_{i}
$$

令下降步长为 $\eta, \eta \in(0,1)$, 则:

$$
	\Delta \omega_{i}=-\frac{\partial E}{\partial \omega_{\mathrm{i}}} * \eta=\eta(\mathrm{y}-\hat{\mathrm{y}}) x_{i}
$$

其中 $\eta \in(0,1)$称为学习率，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。

需注意的是，感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元(functional neuron)，其学习能力非常有限。事实上，上述与、或、非问题都是线性可分(linearly separable)的问题，可以证明[Minsky and Papert, 1969]：

1. 若两类模式是线性可分的，即存在一个线性超平面能将它们分开，则感知机的学习过程一定会收敛(converge)而求得适当的权向量$\mathbf{w}=(w_1,w_2,\ldots,w_{n+1})$；
2. 否则感知机学习过程将会发生振荡(Fluctuation)，$\mathbf{w}$难以稳定下来不能求得合适解，例如感知机甚至不能解决异或这样简单的非线性可分问题。

## 多层网络

要解决非线性可分问题，需要考虑使用多层功能神经元，即神经网络。多层神经网络的拓扑结构如下图所示：

![6.png](https://i.loli.net/2018/10/17/5bc72cbb58ec6.png)

在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“多层前馈神经网络”（multi-layer feedforward neural network），该结构满足以下几个特点：

	* 每层神经元与下一层神经元之间完全互连
	* 神经元之间不存在同层连接
	* 神经元之间不存在跨层连接

![7.png](https://i.loli.net/2018/10/17/5bc72cbb47ff8.png)

根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中不存在环或回路，而不是指该网络只能向前传播而不能向后传播（下节中的BP神经网络正是基于前馈神经网络而增加了反馈调节机制）。神经网络的学习过程就是根据训练数据来调整神经元之间的“连接权”以及每个神经元的阈值，换句话说：神经网络所学习到的东西都蕴含在网络的连接权与阈值中。

