# 二元分类

## **二元分类**

二元分类是监督学习中分类问题的基本应用。监督学习通俗来讲就是训练集拥有正确的标签，例如你想根据房子的尺寸、卧室数等特征预测房价，那么你的训练集中除了有房子的特征以外，还得有这些房子的实际交易价格。与监督学习相对的是无监督学习，无监督学习中的训练集没有正确的标签，就好像你扔给了计算机一大堆数据，让计算机自己去学着找出里面有用的信息。监督学习中主要分为二类模型，回归和分类。生活中有许多二元分类的应用，例如你收到一封邮件，你想判断它是否是垃圾邮件？或者在银行的业务中，银行需要判断是否贷款给某个客户？

下面给出一个二元分类中的图像识别问题的例子。

![img](https://img-blog.csdnimg.cn/20191206192732436.jpg)



假设你有一张图片作为输入，你想识别这张图是否为猫？如果是猫，输出1，否则，输出0，输出结果用y来表示。

那么首先需要知道图片在计算机中是如何表示的？

图像在计算机中以像素为基本单位存储为数据矩阵，最简单的图像就是单通道的灰度图，在灰度图中每个位置(x,y)对应一个灰度值

![img](https://img-blog.csdnimg.cn/2019120620040250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMzExNjA0,size_16,color_FFFFFF,t_70)

如果是彩色RGB图像，那么计算机需要保存三个独立矩阵，分别对应图片在红、绿、蓝三个颜色通道的像素。如果输入的图片是64x64像素的，那就有3个64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度。

![img](https://img-blog.csdnimg.cn/20191206195528329.png)

将所有的像素值取出来放入特征向量$$\vec x$$以表示这张图片，上图中转换的方法是$$x$$为$$(255,231,\ldots,194,202$$ 然后是绿色通道 $$255,134,\ldots,94$$, 最后是蓝色通道 $$255,134,\ldots,142)$$，如果图片是64x64的，那么向量$$\vec x$$的总维度就是$$64\times64\times3=12288$$。用$$n_x$$或$$n$$来表示输入的特征向量x的维度。

在这个二元分类问题中，目标是训练一个分类器。分类器以特征向量x作为输入，然后输出这张图片是1还是0(即是否是猫)。

下面先进行一些符号说明：

$$(x,y)$$表示一个训练样本，$$x$$是$$n_x$$维的特征向量，$$y$$是标签(0或1)。

m表示训练集中训练样本的个数，$$x^{(1)},y^{(1)}$$表示第一个训练样本。

矩阵$$\begin{bmatrix} x^{(1)} &x^{(2)} &...&x^{(m)} \end{bmatrix}$$表示维度为$$n_x\cdot m$$的矩阵，这个矩阵代表所有训练样本的特征向量x，每一列代表一个训练样本的特征向量。$$x\in\mathbb{R}^{n_x\cdot m}$$

矩阵$$\begin{bmatrix} y^{(1)} & y^{(2)} &...&y^{(m)} \end{bmatrix}$$表示维度为$$1\cdot m$$的矩阵，这个矩阵代表所有训练样本的标签，每一列代表一个训练样本的标签。$$x\in\mathbb{R}^{1\cdot m}$$

> 所谓的二分类，就是指输入向量x后返回的标签y的情况只有两种（这个问题中，指0或1）。

## **Logistic回归**

为了解决二元分类问题，下面将介绍[logistic回归](https://so.csdn.net/so/search?q=logistic回归&spm=1001.2101.3001.7020)，它通常用于监督学习问题的输出标签是0或1时，即一个二元分类问题时。

下图是logistic回归算法的结构图示例，其中除以255是数据预处理步骤，这只是为了集中和标准化我们的训练集。

Logistic回归其实是一个很小的神经网络，对于每一张照片我们将它当作输入向量x，然后我们需要给出预测值a，即我们需要找到一种很好的关系来使得我们的预测值尽可能接近实际的标签。而Logistic回归需要不断通过正向传播和反向传播来学习参数(即学习到一种关系)。从输入x到给出预测值的过程称为正向传播，而通过得到的预测值从后往前对w和b进行求导叫做反向传播过程。

![img](https://img-blog.csdnimg.cn/20191206204913380.png)

对于一张图片，将它转换为特征向量x输入算法后，我们期望算法告诉我们这张图是否是一只猫，算法通常会给出一个预测值a，a通常是一个概率值，表示这张图片为正类(即为1)的概率，为了决定这个预测的结果是正类还是负类，我们通常还需要设定一个阈值，任何大于这个阈值的都为正类，否则，为负类。

设logistic回归的参数是w，它的维度等于输入特征向量的维度，即w是一个$$n_x$$维的向量，而b是一个实数。

那么已知输入x和参数w，b，我们如何计算预测值a呢？ 如果我们计算$$a=w_Tx+b$$，即w是x的线性函数。事实上，如果你这么做的话，你其实是在用线性回归算法，线性回归算法并不是一个好的二元分类算法。在此，给出一个不正式的证明，直观上来说，你期望算法的输出a是一个概率，它表示这张图是1的概率，所以a应该是位于0~1之间的，但实际上$$w_Tx+b$$可能是负值，也可能是比1大得多的值，这并不符合我们的要求。所以我们可以对线性回归的值再用一层激活函数，让$$a=\sigma(w_Tx+b)$$，σ函数的图形如下所示：

![img](https://img-blog.csdnimg.cn/20191206210033612.jpg)

直观上可以看到σ函数的值域是0~1，因此这也使得我们的预测值a永远在(0,1)之间。

在logistic回归中，我们需要做的就是根据训练集来训练得到参数w和b的值使得我们的预测输出a尽可能接近真实标签值y。这样，在给出一张新的图片时，我们可以根据参数w和b计算得到a，从而预测出这张新的图片是否是一只猫？