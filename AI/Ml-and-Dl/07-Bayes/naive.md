# 7.3 朴素贝叶斯分类器

- 不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。

- 为了避免这个问题，朴素贝叶斯分类器(Naive Bayes Classifier)采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。在这种情况下，类条件概率（似然）可以改写为

$$
	P(x\mid c)=\prod_{i=1}^d P(x_i\mid c)
$$

这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率：

  - 对于离散属性，可以直接统计频率：
   
$$
	P(x_i\mid c)=\frac{N_{ic}}{N_c}
$$
	
  - 对于连续属性，可以假设属性服从高斯分布（正态分布）：	
   
$$
	P(x_i\mid c)=\frac{1}{\sqrt{2\pi}\sigma_{ic}}\exp\left(-\frac{(x_i-\mu_{ic})^2}{2\sigma_{ic}^2}\right)
$$
	其中$\mu_{ic}$和$\sigma_{ic}$分别是类别$c$下属性$i$的均值和方差。

- 平滑处理(Smoothing)：相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作。然而，若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑处理。
	
其中，拉普拉斯平滑(Laplace Smoothing)是最简单的平滑方法，其实就是在分子上加1，分母加上属性的取值个数：

$$
	\hat P(c)=\frac{N_c+1}{N+|C|}
$$
	

$$
	\hat P(x_i\mid c)=\frac{N_{ic}+1}{N_c+|V_i|}
$$
	
其中$|C|$是类别的个数，$|V_i|$是属性$i$的取值个数。

当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。

# 7.4 半朴素贝叶斯分类器

1. 朴素贝叶斯分类器采用了“属性条件独立性假设”，而在现实任务中这个假设往往很难成立。所以，朴素贝叶斯分类器的一个显著缺点是：当属性之间有相关性时，朴素贝叶斯分类器的性能会下降。

2. 为了解决这个问题，半朴素贝叶斯分类器(Semi-Naive Bayes Classifier)引入了属性选择的概念，即对于每个类别，选择一个属性子集，这个子集中的属性之间是条件独立的，但是属性之间可以有关联。独依赖估计(One-Dependent Estimator, ODE)是半朴素贝叶斯分类器的一种典型方法。
   
$$
	P(c\mid x)\propto P(c)\prod_{i=1}^d P(x_i\mid c,pa_i)
$$
	
其中，$pa_i$是属性$x_i$的父属性集合，$P(x_i\mid c,pa_i)$是在类别$c$下属性$x_i$的条件概率。那么问题就转化为了如何选择父属性集合$pa_i$。
	
1. 选择父属性集合的方法：
   1. “超父”(Super-Parent)：选择一个属性作为所有属性的父属性，即$pa_i=\{x_1,x_2,\cdots,x_{i-1},x_{i+1},\cdots,x_d\}$。
   2. "TAN"(Tree Augmented Naive Bayes)：在训练集上构建一个最大权重生成树，树的每个节点是一个属性，树的边表示属性之间的相关性。在预测时，根据生成树的结构计算条件概率。
   
2. ADOE(Averaged One-Dependent Estimator)：这是一种基于集成学习机制，更为强大的独依赖分类器。ADOE通过随机选择属性子集，构建多个ODE分类器，最后将这些分类器的预测结果进行平均。即：
   
$$
	P(c\mid x)\propto \sum_{i=1,|D_{x_i}|\ge m'}^d P(c,x_i)\prod_{j=1}^d P(x_j\mid c,x_i)
$$

其中，$D_{x_i}$是在第i个属性上取值为$x_i$的样本的集合，$m'$为阈值常数，一般为30。

不难看出，与朴素贝叶斯分类器类似，AODE的训练过程也是“计数”，即在训练数据集上对符合条件的样本进行计数的过程。与朴素贝叶斯分类器相似，AODE无需模型选择，既能通过预计算节省预测时间，也能采取懒惰学习方式在预测时再进行计数，并且易于实现增量学习。