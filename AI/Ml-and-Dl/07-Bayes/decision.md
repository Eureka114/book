# 7.1 贝叶斯决策论

## 贝叶斯分类器

1. 贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。

2. （贝叶斯公式）定理：设试验$E$的样本空间为$S$，$A$为$E$的事件，$B_1,B_2,\cdots,B_n$为样本空间$S$的一个划分，且$P(A)>0,P(B_i)>0(i=1,2,\cdots,n)$，则有
   
$$
	P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\sum\limits_{j=1}^nP(A|B_j)P(B_j)}
$$

若将上述定义中样本空间的划分$B_1,B_2,\cdots,B_n$看作类标，$A$看作一个新的样本，则很容易将条件概率理解为样本$A$属于类标$B_i$的概率。

## 贝叶斯决策论

- 机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“**条件风险**”(Conditional Risk)：
  
$$
  R(c_i|x)=\sum\limits_{j=1}^N\lambda_{ij}P(c_j|x)
$$

其中，$R(c_i|x)$表示在样本$x$下选择类标$c_i$的风险，$\lambda_{ij}$表示将类标$c_j$误判为$c_i$的损失，$P(c_j|x)$表示在样本$x$下类标$c_j$的概率。

- 贝叶斯判定准则(Bayes Decision Rule)：为了最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。（我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和）即：
  
$$
  h^*(x)=\arg\min\limits_{c\in\mathcal{Y}}R(c|x)
$$

此时，我们称$h^*$为贝叶斯最优分类器(Bayes Optimal Classifier)，$R(h^*)$为贝叶斯风险(Bayes Risk)。$1-R(h^*)$反映了分类器的期望分类准确率。如果此时的损失函数是0-1损失函数，我们有：

$$
  R(c|x)=1-P(c|x)
$$


$$
  h^*(x)=\arg\max\limits_{c\in\mathcal{Y}}P(c|x)
$$

即对于每个样本x，选择其后验概率$P(c|x)$最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率$P(c|x)$的问题。

- 后验概率的估计策略：一般来说，我们有两种策略可以对后验概率进行估计：

  1. 判别式模型：直接对$P(c|x)$进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。
  2. 生成式模型：通过先对联合分布$P(x,c)$建模，从而进一步求解$P(c|x)$。贝叶斯分类器就属于生成式模型。

- 先验概率和后验概率：当我们基于贝叶斯定理对后验概率中的$P(c|x)$进行变换，有：
  
$$
  P(c|x)=\frac{P(x,c)}{P(x)}=\frac{P(x|c)P( c )}{P(x)}
$$

其中，$P( c )$为类标$c$的先验(Prior)概率，$P(x|c)$为样本$x$在类标$c$下的条件概率(Class-conditional Probability)，或称作似然(Likelijhood)。$P(x)$是用于归一化的“证据”(Evidence)因子。因此，贝叶斯分类器的核心就是通过先验概率和条件概率来估计后验概率。

> - 先验概率： 根据以往经验和分析得到的概率。
> - 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。
> - （实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。）

- 实际的处理操作：
	1. 对于类先验概率$P( c )$：其就是样本空间中各类样本所占的比例，根据大数定理，当训练样本充足时，$P( c )$可以使用各类出现的频率来代替。
	2. 对于类条件概率$P(x|c)$，它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用**极大似然法**进行估计。

	
	
- 一个实际的应用例：拼写检查。（当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词）

Google的拼写检查基于贝叶斯方法。用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做c（代表correct），拼写错误的情况记做w（代表wrong），那么"拼写检查"要做的事情就是：在发生w的情况下，试图推断出c。换言之：已知w，然后在若干个备选方案中，找出可能性最大的那个c，也就是求$P(c\mid w)$​的最大值。而根据贝叶斯定理，有：

$$
	P(c\mid w)=\frac{P(w\mid c)\cdot P( c )}{P(w)}	
$$

由于对于所有备选的c来说，对应的都是同一个w，所以它们的$P(w)$是相同的，因此我们只要最大化分子即可。其中：

- P( c )表示某个正确的词的出现"概率"，它可以用"频率"代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P( c )就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。

- P(w|c)表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。

所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。

> **大数定律**：是一种描述当试验次数很大时所呈现的概率性质的定律。
>
> **大数定理**：一般是在大数定律的基础上，经过严格的数学证明而得出的结论。它是概率论中的一个重要定理，描述了独立同分布随机变量序列的均值在概率意义下收敛到其数学期望的现象。