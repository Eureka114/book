# 10.5 流形学习

- **流形学习(Metric Learning)**是一种借助拓扑流形概念的降维方法。

- **流形是指在局部与欧式空间同胚的空间**，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种**“邻域保持”**的思想。

- 其中**等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系**，下面将分别对这两种著名的流行学习方法进行介绍。

## 等度量映射（Isomap）

- 等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。

- **因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离**：即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的**Dijkstra算法**或**Floyd算法**计算最短距离，得到高维空间中任意两点之间的距离后便可以使用MDS算法来其计算低维空间中的坐标。

	![13.png](https://i.loli.net/2018/10/18/5bc851b731a1e.png)

- 从MDS算法的描述中我们可以知道：MDS先求出了低维空间的内积矩阵$B$，接着使用特征值分解计算出了样本在低维空间中的坐标，但是并没有给出通用的投影向量$w$，因此对于需要降维的新样本无从下手，书中给出的权宜之计是利用已知高/低维坐标的样本作为训练集学习出一个“投影器”，便可以用高维坐标预测出低维坐标。

- Isomap算法流程如下图：

> 输入：样本集$D=\{x_1,x_2,\ldots,x_m\}$；  
>
> 近邻参数$k$；  
>
> 低维空间维数$d'$  
>
> 过程：  
>
> 1: **for** $i=1,2,\ldots,m$ **do**  
>
> 2: 确定$x_i$的$k$近邻；    
>
> 3: $x_i$与$k$近邻点之间的距离设置为欧式距离，与其他点的距离设置为无穷大；  
>
> 4: **end for**  
>
> 5: 调用最大路径算法计算任意两样本点之间的距离$\text{dist}(x_i,x_j)$；  
>
> 6: 将$\text{dist}(x_i,x_j)$作为MDS算法的输入；  
>
> 7: **return** DMS算法的输出  
>
> 输出：样本集$D$在低维空间的投影$Z=\{z_1,z_2,\ldots,z_m\}$  
>
> 其中1-4的过程就是整个样本集形成一张可达图。  

对于近邻图的构建，常用的有两种方法：**一种是指定近邻点个数**，像kNN一样选取$k$个最近的邻居；**另一种是指定邻域半径**，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：

> 若**邻域范围指定过大，则会造成“短路问题”**，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。  
>
> 若**邻域范围指定过小，则会造成“断路问题”**，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。

## 局部线性嵌入(LLE)

不同于Isomap算法去保持邻域距离，LLE算法试图去保持邻域内的线性关系，假定样本$x_i$的坐标可以通过它的邻域样本线性表出（**线性关系保存不变，即邻域重构系数不变**）：


$$
x_i=w_{ij}x_j+w_{ik}x_k + w_{il}x_l
$$

![16.png](https://i.loli.net/2018/10/18/5bc851b6a7b9a.png)

LLE算法分为两步走，**首先第一步根据近邻关系计算出所有样本的邻域重构系数$w$**：

$$
\begin{array}{cl} \displaystyle \min _{w_1, w_2, \ldots, w_m} & {\displaystyle \sum_{i=1}^{m}\|x_i-\sum_{j \in Q_i} w_{i j} x_j\|_2^2} \\ 
{\text { s.t. }} & {\displaystyle \sum_{j \in Q_i} w_{i j}=1} \end{array} 

$$

其中$x_i$和$x_j$均为已知，令$C_{jk}=(x_i-x_j)^T(x_i-x_k)$，$w_{ij}$有闭式解$\displaystyle w_{i j}=\frac{\displaystyle\sum_{k \in Q_i} C_{j k}^{-1}}{\sum_{l, s \in Q_i} C_{l s}^{-1}}$
。**接着根据邻域重构系数不变，去求解低维坐标**：

$$
\min _{z_1, z_2, \ldots, z_m} \sum_{i=1}^m \|z_i-\sum_{j \in Q_i} w_{i j} z_j\|_2^2
$$

令$Z=(z_1,z_2,\ldots,zz_m) \in \mathbb{R}^{d' \times m},(W)_{ij}=w_{ij}$    $M=(I-W)^T(I-W) \tag{6}$  

这样利用矩阵$M$，优化问题可以重写为：（这里，又一次使用到特征值分解）

$$
\begin{array}{ll} \displaystyle \min_{Z} & \text{tr} (ZMZ^T) \\ 
\text { s.t. } & Z Z^T=I 
\end{array}
$$

$M$特征值分解后最小的$d'$个特征值对应的特征向量组成$Z$，LLE算法的具体流程如下图所示：

> 输入：样本集$D=\{x_1,x_2,\ldots,x_m\}$  
>
> 近邻参数$k$；  
>
> 低维空间维数$d'$  
>
> 过程：  
> 1: **for** $i=1,2,\ldots,m$ **do**
> 
> 2: 确定$x_i$的$k$近邻；
> 
> 3: 从式(5)求得$w_{ij},j \in Q_i$（**局部-邻域线性关系保存不变**）
> 
> 4: 对于$j \notin Q_i$，令$w_{ij}=0$；
> 
> 5: **end for**
> 
> 6: 从式(6)得到$M$
> 
> 7: 对$M$进行特征值分解
> 
> 8: **return** $M$的最小$d'$个特征值对应的特征向量
> 
> 输出：样本集$D$在低维空间的投影$Z=\{z_1,z_2,\ldots,z_m\}$（**和lsomap一样只得到低维坐标**）