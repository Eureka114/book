# 10.3 主成分分析（PCA）

# 10.4 核化主成分分析（KPCA）

## 主成分分析

### 引入

先考虑这样一个问题：对于正交属性空间中的的样本点，如何用一个超平面(直线的高维推广)对所有样本进行恰当的表达？他们必须具有这样的性质：

- 最近重构性：样本点到超平面的距离足够近，即尽可能在超平面附近；
- 最大可分性：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。

这里，十分神奇的是：**最近重构性与最大可分性虽然从不同的出发点来定义优化问题中的目标函数，但最终这两种特性得到了完全相同的优化问题**。

### 主成分分析

- 主成分分析：主成分分析(PCA, )直接通过一个线性变换，将原始空间中的样本投影到新的低维空间中。
- 简单来理解这一过程便是：**PCA采用一组新的基来表示样本点，其中每一个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。**

$$
\begin{aligned}
        \sum_{i=1}^m \| {\sum_{j=1}^{d'} z_{i j} w_j-x_i} \|_2^2 
        &=\sum_{i=1}^m z_i^T z_i-2 \sum_{i=1}^m z_i^T W^T x_i+\text { const } \\ 
        & \propto -\text{tr} \left(W^T \left(\sum_{i=1}^m x_i x_i^T\right) W\right) 
    \end{aligned}
$$

基于最近重构性，可得基于最大可分性的式子：

$$
    \begin{aligned}
        \min_W \text{tr}(W^T X X^T W)\\
        s.t. W^T W = I
    \end{aligned}
$$

接着使用拉格朗日乘子法求解上面的优化问题，得到：$X X^T W=\lambda W$，其中$XX^T$即$X$中心化后的协方差矩阵。  
因此只需对协方差矩阵进行特征值分解即可求解出$W$，PCA算法的整个流程如下图所示：

> 输入：样本集$D=\{x_1,x_2,\ldots,x_m\}$；  
> 低维空间维数$d'$  
> 过程：
> 1: 对所有样本进行中心化：$\displaystyle x_i \leftarrow x_i - \frac{1}{m} \sum_{i=1}^m x_i$;  
> 2: 计算样本的协方差矩阵$XX^T$  
> 3: 对协方差矩阵$XX^T$做特征值分解；  
> 4: 取最大的$d'$个特征值所对应的特征向量$w_1,w_2,\ldots,w_{d'}$  
> 输出：投影矩阵$W=(w_1,w_2,\ldots,w_{d'})$（**得出了投影矩阵，对于新样本只需乘上投影矩阵即可**）

另一篇博客给出更通俗更详细的理解：[主成分分析解析（基于最大方差理论）](http://blog.csdn.net/u011826404/article/details/57472730)



## 核化线性降维

SVM在处理非线性可分时，通过引入核函数将样本投影到高维特征空间，接着在高维空间再对样本点使用超平面划分。这里也是相同的问题：若我们的样本数据点本身就不是线性分布，那还如何使用一个超平面去近似表出呢？因此也就引入了核函数，**即先将样本映射到高维空间，再在高维空间中使用线性降维的方法**。下面主要介绍**核化主成分分析（KPCA）**的思想。  
若核函数的形式已知，即我们知道如何将低维的坐标变换为高维坐标，这时我们只需先将数据映射到高维特征空间，再在高维空间中运用PCA即可。但是一般情况下，我们并不知道核函数具体的映射规则，例如：Sigmoid、高斯核等，我们只知道如何计算高维空间中的样本内积，这时就引出了KPCA的一个重要创新之处：**即空间中的任一向量，都可以由该空间中的所有样本线性表示**。证明过程也十分简单：

$$
    \left(\sum_{i=1}^m z_i z_i^T \right) W = \lambda W
$$

其中$z_i$为样本点在高斯特征空间中的坐标向量，$W$为高斯特征空间中的一个新基向量。

$$
    \begin{aligned}
        W &=\frac{1}{\lambda}\left(\sum_{i=1}^m z_i z_i^T \right) W
        =\sum_{i=1}^m z_i \frac{z_i^T W}{\lambda}=\sum_{i=1}^m z_i \alpha_i
    \end{aligned}
$$

这样我们便可以将高维特征空间中的投影向量$w_i$使用所有高维样本点线性表出，接着代入PCA的求解问题。

> 求解如下：
>
> $$
>   \begin{aligned}
>       \Phi(X)\Phi(X)^Tw_i &= \lambda_i w_i \\
>       w_i &= \sum_{k=1}^N \alpha_i \Phi(x_i) = \Phi(X) \alpha\\
>       \Phi(X) \Phi(X)^T \Phi(X) \alpha &= \lambda_i \Phi(X) \alpha\\
>       \Phi(X)^T \Phi(X) \Phi(X) \Phi(X)^T \alpha &= \lambda_i \Phi(X)^T \Phi(X) \alpha\\
>       K^2 \alpha &= \lambda_i K \alpha\\
>       K \alpha &= \lambda_i \alpha\\
>   \end{aligned}
> $$
>
> 空间中任一向量，都可以由该空间中的所有样本线性表示，$\Phi(X)^T \Phi(X)$是核函数矩阵。

- 化简到最后一步，发现结果十分的美妙，只需对核矩阵$K$进行特征分解，便可以得出投影向量$w_i$对应的系数向量$\alpha$，因此选取特征值前$d'$大对应的特征向量便是$d'$个系数向量。这时对于需要降维的样本点，只需按照以下步骤便可以求出其降维后的坐标。

- 可以看出：KPCA在计算降维后的坐标表示时，需要与所有样本点计算核函数值并求和，因此该算法的计算开销十分大。


$$
    \begin{aligned} 
        \hat{x}_{new}  
        &= w_i^T x_{new} \\
        &= \left(\sum_{i=1}^N \Phi(x_i)\alpha \right)^T \Phi(x_{new}) \\
        &= (\Phi(X)\alpha)^T \Phi(x_{new}) \\
        &= \alpha^T \Phi(X)^T \Phi(x_{new}) \\
        &= [\alpha_1,\alpha_2,\ldots,\alpha_N][k(x_1,x_{new}),k(x_2,x_{new}),\ldots,k(x_N,x_{new})]^T
    \end{aligned}
$$

其中$w_i^T x_{new}$表示新样本点在$w_i$维度上的投影坐标共有$d'$个投影向量$w$
