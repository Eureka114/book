# 8.3 Bagging与随机森林

## 简介

Bagging和随机森林都是并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行。上面已经提到，产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思想，都是通过“自助采样”的方法来增加多样性。

## Bagging

- Bagging (Bootstrap Aggregating)由Breiman在1996年提出，是并行式集成学习方法最著名的代表。这个方法基于自助采样法。

- Bagging使用“有放回”采样的方式选取训练集。对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。

	> 为什么是36.8%？
	>
	> 
	> $$
	> \lim_{m\to\infty}\left(1-\frac{1}{m}\right)^m=\frac{1}{\mathrm{e}}\approx0.368
	> $$
	> 

- Bagging算法流程如下：

	![8.png](https://i.loli.net/2018/10/18/5bc84d0d0e761.png)

	可以看出：

	- Bagging主要通过**样本的扰动**来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。
	- 从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。
	- 同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：**AdaBoost关注于降低偏差，而Bagging关注于降低方差。**

## 随机森林

- 随机森林 (Random Forest, RF)是由Breiman于2001年提出的一个Bagging的扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。

- RF的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种**属性扰动**，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐$K=\log_2d$。

- 这样，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。

	![9.png](https://i.loli.net/2018/10/18/5bc84d0d7a4fd.png)

