# 8.5 多样性

## 误差——分歧分解

![](https://img2.imgtp.com/2024/05/27/ntMxst44.png)

- 然而，我们很难甚至不能对$E=\overline E-\overline A$作为优化目标来求解，不仅由于于它们是定义在整个样本空间上，还由于$\overline A$不是一个可直接操作的多样性度量，它仅在集成构造好之后才能进行估计。
- 此外，上述的内容只能用在回归学习上，不能推广到分类学习任务上去。



## 多样性度量

- 多样性度量(Diversity Measure)是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。典型做法是考虑个体分类器的两两相似/不相似性。
- 常见的“成对型”多样性度量：
	- 不合度量
	- 相关系数
	- Q-统计量
	- $\kappa$-统计量



## 多样性增强

在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。


> **数据样本扰动**，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。
>
> **输入属性扰动**，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。
>
> **输出表示扰动**，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。
>
> **算法参数扰动**，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。