# 8.1 个体与集成

## 何谓集成学习？

集成学习(Ensemble Learning)：是一种机器学习范式，它通过构建并结合多个学习器（也称为基学习器或组件学习器）来完成学习任务。这些学习器可以是从同一种学习算法产生的同质学习器，也可以是从不同学习算法产生的异质学习器。集成学习的核心思想是“好而不同”，即基学习器应该具有好的性能，并且它们之间的预测结果应该具有差异性，以提高整体的泛化性能。一般来讲，集成学习结合后的“学习器”的泛化性能要由于任何一个学习器。

集成学习有时也被称为多分类器系统(Multi-classifier System)、基于委员会的学习(Committee-based Learning)等。

## 个体与集成

集成学习的基本结构：先产生一组个体学习器，再使用某种策略将它们结合在一起。若所有的个体学习器都属于同一类别，则称该集成为同质的(Homogeneous)；若个体学习器包含多种类型的学习算法，则称该集成为异质的(Hetergenemous)。

  ![1.png](https://i.loli.net/2018/10/18/5bc84d0c15683.png)

同质集成和异质集成：在同质集成中，个体学习器被称为“基学习器”(Base Learner)，对应的学习算法为“基学习算法”(Base Learning Algorithm)。而在异质集成中，个体学习器称为“组件学习器”(Component Learner)，或直称为“个体学习器”。

集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”(Weak Learner)尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的。

## 准确性和多样性

如何获得比最好的单一学习器更好的性能呢？这里，引出了集成学习的两个概念：准确性和多样性。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。一般来说，准确度和差异度都较高的情况下，可以较好地提成集成性能。

  ![2.png](https://i.loli.net/2018/10/18/5bc84d0d23e13.png)

分析：

> 考虑二分类问题$y\in\{-1,+1\}$和真实函数$f$，假定基分类器的错误率为$\epsilon$，则对每一个基分类器$h_i$，有：
>
> 
> $$
> P(h_i(x)\neq f(x))=\epsilon
> $$
> 
>
>  假设集成通过简单投票法结合T个基分类器（假设T为奇数），若有超过半数的基分类器正确，则集成分类就正确，即
>
> 
> $$
> H(x)=\text{sign}\left(\sum_{i=1}^T h_i(x)\right)
> $$
> 
>
> 如果再假设基分类器的错误率**互相独立**，则由Hoeffding不等式得，集成的错误率为：
>
> 
> $$
> \begin{aligned}
>     P(H(x)\neq f(x))&=\sum_{k=0}^{\lfloor T/2 \rfloor}\binom{n}{k}(1-e)^k\epsilon^{T-k}\\
>     &\le \exp\left(-\frac{1}{2}T(1-2\epsilon)^2\right)
>   \end{aligned}
> $$
> 

此时，集成器错误率随着基分类器的个数的增加呈指数下降。但前提是基分类器之间**相互独立**，在实际情形中显然是不可能的，因为这一系列的分类器都是为了解决相同问题而进行训练的，因此在预测新样本时存在着很大的联系。

因此，**个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量**，准确性高意味着牺牲多样性，所以产生“**好而不同**”的个体学习器正是集成学习研究的核心。

## 主流集成学习方法

Boosting, Bagging, 随机森林。
