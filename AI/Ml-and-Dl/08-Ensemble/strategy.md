# 8.4 结合策略

## 简介

- 学习器结合的好处：
	1. （统计方面）结合多个学习器可以在很大的假设空间上的学习中减小导致泛化性能不佳的风险；
	2. （计算方面）通过多次运行，可以降低陷入比较糟糕的局部极小点的风险；
	3. （表示方面）某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，使用结合多个学习器，有可能学到更好的近似。
- 以下的介绍，都是建立在假定集成包含$T$个基学习器$\{h_1,h_2.\ldots,h_r\}$，其中，$h_i$在示例$x$上的输出为$h_i(x)$的基础上的。

## 常见策略

### 平均法（回归问题）

- 对数值型输出$h_i(x)\in\mathbb{R}$，最常见的结合策略是使用平均法(averaging)。其中，有简单平均法和加权平均法。


$$
H(x)=\sum_{i=1}^T w_ih_i(x),\quad w_i\ge0,\sum w_i=1
$$


- 易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，**一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法**。（现实任务中的训练样本通常不充分或存在噪声，这将使得学出的权重不完全可靠。尤其是对规模比较大的集成来说，要学习的权重比较多，较容易导致过拟合。）

### 投票法 （分类问题）

- 绝对多数投票法 (Majority Voting)：


$$
H(x)=\begin{cases}c_{j}, & \text { if } \sum\limits_{i=1}^{T} h_{i}^{j}(x)>0.5 \sum\limits_{k=1}^{N} \sum\limits_{i=1}^{T} h_{i}^{k}(x) \\
\text { reject, } & \text { otherwise. }\end{cases}
$$


- 绝对多数投票法提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。
- 相对多数投票法 (Plurality Voting)：


$$
H(x)=c\arg\max_j\sum\limits_{i=1}^Th_i^j(x)
$$


- 加权投票法 (Weighted Voting)：（其中，$w_i$是权重。）


$$
H(x)=c\arg\max_j\sum\limits_{i=1}^Tw_ih_i^j(x)
$$


- 对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。
	- 类标记：$h_i^j(x)=\{0,1\}$，若$h_i$将样本$x$预测为$c_j$则取值为1，否则为0。使用此类型的投票方法称为“硬投票”。
	- 类标记：$h_i^j(x)=(0,1)$，相当于对后验概率$P(c_j\mid x)$的一个估计使用此类型的投票方法称为“软投票”。
- 一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，**一般基于类概率进行结合往往比基于类标记进行结合的效果更好**，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。



### 学习法

学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个$m\times T$的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：**投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果**。

![16.png](https://i.loli.net/2018/10/18/5bc84de25cbaf.png)