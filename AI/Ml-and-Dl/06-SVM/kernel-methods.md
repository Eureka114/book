# 6.6 核方法

1. 回顾前文可以发现，若给定训练样本$\{(x_i,y_i)\}$，且不考虑偏移项b，则无论是SVM还是SVR，学得得模型总能表示成核函数$\kappa(x,x_i)$的线性组合。不仅如此，事实上，我们有一个更一般的结论：
2. **表示定理**：令$\mathbb{H}$为核函数$\kappa$对应的再生核希尔伯特空间，$\lVert h \rVert _{\mathbb{H}}$表示$\mathbb{H}$空间中关于h的范数，$\mathbb{H}$中的任意函数$f(x)$都可以表示成核函数$\kappa(x,x_i)$的线性组合，即：

$$
   f(x)=\sum_{i=1}^{m}\alpha_i\kappa(x,x_i)
$$

其中$\alpha_i$是$\mathbb{H}$空间中的系数。

1. 表示定理对损失函数没有限制，对正则化项$\Omega$仅要求单调递增，甚至不要求其是凸函数，意味着对于一般的损失函数和正则化项，优化问题的最优解都可以表示为核函数$\kappa(x,x_i)$的线性组合；这显示出核函数的巨大威力。
2. **核方法**：核方法是指通过核函数将输入空间映射到一个更高维的特征空间，从而使得原本线性不可分的问题在新的特征空间中变得线性可分。核方法的基本思想是利用核函数$\kappa(x,x_i)$来隐式地定义一个高维特征空间，而不需要显式地计算出映射后的特征向量，从而避免了维度灾难问题。
3. **核线性判别分析**：核线性判别分析（Kernel Linear Discriminant Analysis，KLDA）是核方法的一个典型应用。KLDA是线性判别分析（LDA）的核化扩展，通过核函数将输入空间映射到一个更高维的特征空间，从而使得原本线性不可分的问题在新的特征空间中变得线性可分。