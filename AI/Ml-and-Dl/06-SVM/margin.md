# 6.1 间隔与支持向量

## 间隔

1. 分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同的类别分隔开。直观上看，如在二分类问题中，我们应该去找位于两类训练样本“正中间”的划分超平面，因为它所产生的分类结果是最为鲁棒的，对未见示例的泛化能力越强。

2. 常用的间隔有两个，一种称之为函数间隔，一种为几何间隔。在支持向量机中使用的是几何间隔。

3.  **函数间隔**：对于给定的训练数据集$D=\{(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),\cdots,(\mathbf{x}_N,y_N)\}$，定义超平面$\mathbf{w}^T\mathbf{x}+b=0$关于样本点$(\mathbf{x}_i,y_i)$的函数间隔为

$$
   \hat{\gamma}_i=y_i(\mathbf{w}^T\mathbf{x}_i+b)=y_i f(x)
$$

定义超平面$\mathbf{w}^T\mathbf{x}+b=0$关于训练数据集$D$的函数间隔为

$$
   \hat{\gamma}=\min\hat{\gamma}_i, (i=1,2,\ldots,n)
$$

显然，对于误分类的样本点$(\mathbf{x}_i,y_i)$，其函数间隔为负。而且，函数间隔的大小依赖于$\mathbf{w}$和$b$的比例尺度，如果$\mathbf{w}$和$b$同时成倍缩放，超平面并没有改变，但函数间隔却成倍增大。因此，函数间隔并不能很好地表示样本点$x_i$距离超平面的远近。因此，我们使用数据点到超平面的真实距离作为间隔的度量。
1. **几何间隔**：在样本空间中，划分超平面可以通过线性方程$\mathbf{w}^T\mathbf{x}+b=0$来进行描述。其中，$w$为法向量，$b$为位移项。样本空间中任意一点$x$到超平面的距离可写为

$$
   \gamma=\frac{\mathbf{w}^T\mathbf{x}+b}{||\mathbf w||}
$$

此时，为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：

$$
   \tilde{\gamma}_i=y_i\left(\frac{\mathbf{w}^T\mathbf{x}_i+b}{||\mathbf w||}\right)
$$

假设平面$\mathbf{w}^T\mathbf{x}+b=0$能将训练数据集正确分类，即对所有的$(\mathbf{x}_i,y_i)$，若$y_i=+1$，有$y_i(\mathbf{w}^T\mathbf{x}_i+b)>0$；若$y_i=-1$，有$y_i(\mathbf{w}^T\mathbf{x}_i+b)<0$。
## 支持向量和最大间隔
- 此时，可以做一次伸缩变换，使得上述等式中的右侧为1。距离超平面最近的这几个训练样本点使得上述等式成立，即
   
$$
   y_i(\mathbf{w}^T\mathbf{x}_i+b)=\pm 1
$$

这些训练样本点被称为支持向量(Support Vector)。而两个异类支持向量到超平面的距离之和为

$$
   \gamma=\frac{2}{||\mathbf w||}
$$

被称为间隔(margin)。

- 支持向量机的学习问题就是求解能够正确划分训练数据集并且使得间隔最大的划分超平面。即
   
$$
   \max_{w,b} \frac{2}{||w||}\text{ s.t. }y_i(\mathbf{w}^T\mathbf{x}_i+b)\geq 1, i=1,2,\ldots,m
$$

又因为最大化$||w||^{-1}$，等价于最小化$\frac{1}{2}||w||^2$，因此，上式重写为

$$
   \min_{w,b} \frac{1}{2}||w||^2\text{ s.t. }y_i(\mathbf{w}^T\mathbf{x}_i+b)\geq 1, i=1,2,\ldots,m
$$

这是一个凸二次规划问题，可以用现成的优化计算包求解。这就是支持向量机的基本型。
## 支持向量机
支持向量机(Support Vector Machine)是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。